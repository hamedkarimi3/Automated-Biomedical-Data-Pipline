# Automated-Biomedical-Data-Pipline
A scalable ETL pipline for bimedical research data processing Using Apache Airflow, spark, and Snowflake
Automated Data Pipeline for Biomedical Research
ğŸ“ Organization: Karolinska Institute
ğŸ“… Timeline: 2024â€“2025

ğŸ“– Overview
This project builds an end-to-end automated ETL pipeline to process omics and patient data at Karolinska Institute. It optimizes biomedical research workflows by integrating real-time and batch data processing.

ğŸ¯ Key Features
âœ… ETL pipeline using Apache Airflow for orchestration
âœ… Data warehouse built in Snowflake for scalable storage
âœ… Real-time streaming integration with Apache Kafka
âœ… Automated data validation using Great Expectations
âœ… Data visualization with Power BI/Tableau

ğŸ› ï¸ Tech Stack
Data Ingestion: Apache Kafka, REST APIs, Python
Processing: Apache Airflow, Spark (PySpark), SQL
Storage: Snowflake, Azure Blob Storage
Data Quality: Great Expectations, dbt
Visualization: Power BI, Tableau
ğŸ“Š Project Impact
ğŸš€ 25% improvement in data processing efficiency
ğŸ” 40% reduction in manual research data processing
ğŸ”’ 100% GDPR compliance for patient data security