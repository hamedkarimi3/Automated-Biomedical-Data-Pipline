# Automated-Biomedical-Data-Pipline
A scalable ETL pipline for bimedical research data processing Using Apache Airflow, spark, and Snowflake
Automated Data Pipeline for Biomedical Research
📍 Organization: Karolinska Institute
📅 Timeline: 2024–2025

📖 Overview
This project builds an end-to-end automated ETL pipeline to process omics and patient data at Karolinska Institute. It optimizes biomedical research workflows by integrating real-time and batch data processing.

🎯 Key Features
✅ ETL pipeline using Apache Airflow for orchestration
✅ Data warehouse built in Snowflake for scalable storage
✅ Real-time streaming integration with Apache Kafka
✅ Automated data validation using Great Expectations
✅ Data visualization with Power BI/Tableau

🛠️ Tech Stack
Data Ingestion: Apache Kafka, REST APIs, Python
Processing: Apache Airflow, Spark (PySpark), SQL
Storage: Snowflake, Azure Blob Storage
Data Quality: Great Expectations, dbt
Visualization: Power BI, Tableau
📊 Project Impact
🚀 25% improvement in data processing efficiency
🔍 40% reduction in manual research data processing
🔒 100% GDPR compliance for patient data security